{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Binary Classification Example\n",
    "\n",
    "From the book [\"Deep Learning with Python\"](https://www.manning.com/books/deep-learning-with-python) by Francois Chollet\n",
    "\n",
    "Two-class classification or binary classification is some of the most common types of machine learning problems an analysts will encounter in the real world. I have used this method in sales operations to look at what sales opportunities are likley to close to seeing when a customer is likely to leave in what is know as churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The IMDb Data-Set\n",
    "\n",
    "The Internet Movie Database is an online database that houses information about movies, televsion shows and video games. This dataset has a set of 50K highly polarized reviews. The reviews are spilt into 25K reviews for training and 25K for testing. Each set contains a review mix of 50% positive and 50% negative.\n",
    "\n",
    "This data has already been preprocessed. The reviews have been turned into sequences of integers. Each integer repersents a word in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "\n",
    "This data comes prepackaged with Keras. You can load the code with the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the `num_words = 10000` takes the top 10K words by frequency in the training data, basically meaning the rare words will be removed. This means that our data will more of a managable size to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take A Quick Look\n",
    "\n",
    "`train_data` and `test_data` are lists of reviews. So each review is a list of word indices with an encoding of a sequence of words. The labels `train_labels` and `test_labels` are lists of 0s and 1s where 0 stands for *negative* review and 1 is a *positive* review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 47,\n",
       " 8,\n",
       " 30,\n",
       " 31,\n",
       " 7,\n",
       " 4,\n",
       " 249,\n",
       " 108,\n",
       " 7,\n",
       " 4,\n",
       " 5974,\n",
       " 54,\n",
       " 61,\n",
       " 369,\n",
       " 13,\n",
       " 71,\n",
       " 149,\n",
       " 14,\n",
       " 22,\n",
       " 112,\n",
       " 4,\n",
       " 2401,\n",
       " 311,\n",
       " 12,\n",
       " 16,\n",
       " 3711,\n",
       " 33,\n",
       " 75,\n",
       " 43,\n",
       " 1829,\n",
       " 296,\n",
       " 4,\n",
       " 86,\n",
       " 320,\n",
       " 35,\n",
       " 534,\n",
       " 19,\n",
       " 263,\n",
       " 4821,\n",
       " 1301,\n",
       " 4,\n",
       " 1873,\n",
       " 33,\n",
       " 89,\n",
       " 78,\n",
       " 12,\n",
       " 66,\n",
       " 16,\n",
       " 4,\n",
       " 360,\n",
       " 7,\n",
       " 4,\n",
       " 58,\n",
       " 316,\n",
       " 334,\n",
       " 11,\n",
       " 4,\n",
       " 1716,\n",
       " 43,\n",
       " 645,\n",
       " 662,\n",
       " 8,\n",
       " 257,\n",
       " 85,\n",
       " 1200,\n",
       " 42,\n",
       " 1228,\n",
       " 2578,\n",
       " 83,\n",
       " 68,\n",
       " 3912,\n",
       " 15,\n",
       " 36,\n",
       " 165,\n",
       " 1539,\n",
       " 278,\n",
       " 36,\n",
       " 69,\n",
       " 2,\n",
       " 780,\n",
       " 8,\n",
       " 106,\n",
       " 14,\n",
       " 6905,\n",
       " 1338,\n",
       " 18,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 610,\n",
       " 40,\n",
       " 6,\n",
       " 87,\n",
       " 326,\n",
       " 23,\n",
       " 2300,\n",
       " 21,\n",
       " 23,\n",
       " 22,\n",
       " 12,\n",
       " 272,\n",
       " 40,\n",
       " 57,\n",
       " 31,\n",
       " 11,\n",
       " 4,\n",
       " 22,\n",
       " 47,\n",
       " 6,\n",
       " 2307,\n",
       " 51,\n",
       " 9,\n",
       " 170,\n",
       " 23,\n",
       " 595,\n",
       " 116,\n",
       " 595,\n",
       " 1352,\n",
       " 13,\n",
       " 191,\n",
       " 79,\n",
       " 638,\n",
       " 89,\n",
       " 2,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 106,\n",
       " 607,\n",
       " 624,\n",
       " 35,\n",
       " 534,\n",
       " 6,\n",
       " 227,\n",
       " 7,\n",
       " 129,\n",
       " 113]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not have a word value that goes over 10K in our `train_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these numbers in english? Below is a decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index() #word_index is a dictionary mapping\n",
    "reverse_word_index = dict(\n",
    "[(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "reverse_word_index.get(i - 3, '?') for i in train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Preperation\n",
    "\n",
    "We can not use list of integers into a neural network. These lists need to be transformed into tensors. There are a couple of ways in which to do this.\n",
    "1. Pad the lists to ensure that they are all the same length. We would then use that as the first layer in the network. This would be known as an Embedding Layer.\n",
    "2. We could One-hot encode the lists to turn them into vectors of 0's and 1's. Basically if we take the sequence [3, 5] into a 10K dimensional vector that would be all 0's with exception of of indices 3 and 5, which would be 1s. We could then use the first layer in the network as a Dense Layer.\n",
    "\n",
    "We going to move forward with 2. Vectorize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension)) #all zero matrix\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. # Indices of results to 1's\n",
    "    return results\n",
    "# Vectorize train and test data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the samples look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct The Network\n",
    "\n",
    "Our input data is vectors, and the labels are scalars (1's and 0's). A type of network that does well on this problem is a simple stack of fully connected dense layers with relu activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hold on! What is / are relu activations?\n",
    "\n",
    "ReLU, or Rectitified Linear Unit is one of the most commonly used activation functions in deep learning models. Basically the function returns 0 if it gets any negative input. \n",
    "\n",
    "Any positive value *x* it returns that value back.\n",
    "\n",
    "An activation function serves two purposes:\n",
    "1. This helps the model account for interaction effects. *Interactive effects* are when one variable affects a prediction differently depending on the value of another variable. \n",
    "2. It also helps the model account for non-linear effects. The effect of increasing the predictor by one is different at different values of that predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are passing an arguement to each Dense Layer (16). This is the number of hidden units of the layer. A hidden layer is the dimesnion of the representation space of the layer.\n",
    "\n",
    "Now, back to ReLU activation. The relu implements the following chain of tensor operations:\n",
    "\n",
    "`output = relu(dot(W, input) + b)`\n",
    "\n",
    "With 16 hidden units means that the weight matrix `W` will have shape `(input_dimension, 16)`, the dot product with `W` will project the input data onto a 16-dimensional repersentation space, we also add the bias vector `b`and apply the ReLU operation.\n",
    "\n",
    "Bascially we are giving our young model some freedom with learning internal representaitons. Having more hidden units allows your network to learn more complex representations. This comes at a cost. This could make a network more computationally expensive and could lead to learning unwanted patterns that yes, will help on the training data, but hurt when it comes to test time.\n",
    "\n",
    "We have to key architecture decisions to be made about the stack of dense layers:\n",
    "1. How many layers do we use?\n",
    "2. How many hidden units to chose for each layer.\n",
    "\n",
    "So, for this dataset we are going to use two intermediate layers and a 3rd layer that will output the scalar prediction of the sentiment of the current review.\n",
    "\n",
    "The intermediate layers will use ReLU as their activation function, and for the final layer we will use a sigmoid activation to produce a probaility between 0 and 1 for how likely the review is to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to pick a **loss function** and an **optimizer** Since we are facing a binary classfication problem and the output of the network is a probability. It would be best to use `binary_crossentropy` loss. This is not the only choice. We could also use `mean_squared_error`. But crossentropy is usually the best choice when you are dealing with models that output probabilties. Crossentropy is a quantity from the field of Information Theory that measures the \"distance\" between probaility distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.001),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "To monitor the accuracy on our training of the model on data it has never seen before we are going to create a validation set by setting apart 10K samples from the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train our model for 20 epochs. This means 20 interations over all samples in the x_train and y_train tensors. In mini-batches of 512 samples. At this time we will monitor loss and accuracy on the 10K samples that we set apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 5s 302us/step - loss: 0.4977 - acc: 0.7951 - val_loss: 0.3717 - val_acc: 0.8724\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 120us/step - loss: 0.2958 - acc: 0.9044 - val_loss: 0.2990 - val_acc: 0.8907\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.2160 - acc: 0.9285 - val_loss: 0.3084 - val_acc: 0.8718\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.1742 - acc: 0.9434 - val_loss: 0.2830 - val_acc: 0.8842\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 107us/step - loss: 0.1415 - acc: 0.9543 - val_loss: 0.2863 - val_acc: 0.8850\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 97us/step - loss: 0.1143 - acc: 0.9653 - val_loss: 0.3094 - val_acc: 0.8809\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 97us/step - loss: 0.0970 - acc: 0.9710 - val_loss: 0.3146 - val_acc: 0.8840\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0803 - acc: 0.9765 - val_loss: 0.3869 - val_acc: 0.8658\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.0658 - acc: 0.9819 - val_loss: 0.3651 - val_acc: 0.8778\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.0556 - acc: 0.9854 - val_loss: 0.3862 - val_acc: 0.8794\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.0449 - acc: 0.9889 - val_loss: 0.4177 - val_acc: 0.8764\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0386 - acc: 0.9913 - val_loss: 0.4525 - val_acc: 0.8697\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0301 - acc: 0.9932 - val_loss: 0.4713 - val_acc: 0.8732\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0244 - acc: 0.9950 - val_loss: 0.5024 - val_acc: 0.8720\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 92us/step - loss: 0.0184 - acc: 0.9975 - val_loss: 0.5335 - val_acc: 0.8693\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 92us/step - loss: 0.0163 - acc: 0.9979 - val_loss: 0.5710 - val_acc: 0.8699\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 92us/step - loss: 0.0110 - acc: 0.9990 - val_loss: 0.6192 - val_acc: 0.8651\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0121 - acc: 0.9971 - val_loss: 0.6420 - val_acc: 0.8669\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 92us/step - loss: 0.0057 - acc: 0.9997 - val_loss: 0.7249 - val_acc: 0.8569\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0107 - acc: 0.9975 - val_loss: 0.7016 - val_acc: 0.8657\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 512,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'yo', label = 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label = 'Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
